{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5f5e3ff98>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 501:\n",
      "Image - Min Value: 21 Max Value: 248\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 8 Name: ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmKIHgML+D0UhCihFpLe7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23a76aaccf8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "helper.display_stats(cifar10_dataset_folder_path, 1, 501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In batch 1, image with sample_id 5 has label_id = 1, Automobile.   \n",
    "In batch 1, image with sample_id 6 has label_id = 2, Bird.    \n",
    "In batch 1, image with sample_id 0 has label_id = 6, Frog.   \n",
    "All possible labels are 0,1,2,3,4,5,6,7,8,9 and the labels are not in any particular order. \n",
    "The minimum value of an image seems to be 0, while maximum is 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #length = np.linalg.norm(x)  #32x32x3 array with values 0 - 255\n",
    "    \n",
    "    #return x/length\n",
    "    return np.divide(x, 255.0)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "    encoded_data = lb.transform(x)\n",
    "    \n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function   #batch_sizex32x32x3 \n",
    "    return tf.placeholder(tf.float32, shape=(None, image_shape[0],image_shape[1],image_shape[2]), name = 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function #batch_sizex10\n",
    "    return tf.placeholder(tf.float32, shape = (None, n_classes), name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer, output depth\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer, filter height and filter width\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    N_layer = conv_ksize[0]*conv_ksize[1]*x_tensor.get_shape().as_list()[3] \n",
    "    \n",
    "    filter_weights = tf.Variable(tf.truncated_normal((conv_ksize[0], \n",
    "                                                      conv_ksize[1], x_tensor.get_shape().as_list()[3], \n",
    "                                                      conv_num_outputs),\n",
    "                                                     stddev=tf.sqrt(2.0/float(N_layer)))) \n",
    "    # (filter height, filter width, input_depth, output_depth)\n",
    "    filter_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, filter_weights, \n",
    "                              strides=[1,conv_strides[0],conv_strides[1],1], padding='SAME')\n",
    "    conv_layer2 = tf.nn.bias_add(conv_layer, filter_bias)\n",
    "    conv_layer3 = tf.nn.relu(conv_layer2)\n",
    "    \n",
    "    conv_layer4 = tf.nn.max_pool(conv_layer3,[1,pool_ksize[0],pool_ksize[1],1],\n",
    "                                [1,pool_strides[0],pool_strides[1],1],padding='SAME')\n",
    "\n",
    "    return conv_layer4 \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions. (32,32,3)\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.get_shape().as_list()  #[batch_size,32,32,3]\n",
    "    flat = tf.reshape(x_tensor, [-1, int(shape[1])*int(shape[2])*int(shape[3])])\n",
    "    return flat\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function   \n",
    "    N_full_layer = x_tensor.get_shape().as_list()[1]\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs],\n",
    "                                             stddev=tf.sqrt(2.0/float(N_full_layer))))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs)) \n",
    "    conn_output = tf.add(tf.matmul(x_tensor, weight),bias)\n",
    "\n",
    "    return conn_output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Out_layer = x_tensor.get_shape().as_list()[1]\n",
    "    weight_out = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs],\n",
    "                                             stddev=tf.sqrt(2.0/float(Out_layer))))\n",
    "    bias_out = tf.Variable(tf.zeros(num_outputs)) \n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weight_out),bias_out)\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layers = 3\n",
    "    conv_outputs = [16, 32, 64]\n",
    "    conv_ksizes = [[4, 4], [4, 4], [4, 4]]\n",
    "    conv_strides = [[1, 1], [1, 1], [1, 1]]\n",
    "    \n",
    "    pool_ksizes = [[2, 2], [2, 2], [2, 2]]\n",
    "    pool_strides = [[2, 2], [2, 2], [2, 2]]\n",
    "\n",
    "    conv_layer = x \n",
    "    for i in range(conv_layers):\n",
    "        conv_layer = conv2d_maxpool(conv_layer,conv_outputs[i],\n",
    "                                    conv_ksizes[i],conv_strides[i],pool_ksizes[i],pool_strides[i])\n",
    "        conv_layer = conv2d_maxpool(conv_layer,conv_outputs[i],\n",
    "                                    conv_ksizes[i],conv_strides[i],pool_ksizes[i],pool_strides[i])\n",
    "        conv_layer = tf.nn.dropout(conv_layer, 0.75*keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat_layer = flatten(conv_layer)\n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    #model = fully_conn(model, 512)\n",
    "    fully_conn_layer=flat_layer\n",
    "    #full_outputs = [256,128]\n",
    "    #num_fully_conn_layers = 2\n",
    "    #fully_num_outputs = 10\n",
    "    #for i in range(num_fully_conn_layers):\n",
    "    #    fully_conn_layer = fully_conn(fully_conn_layer, full_outputs[i])\n",
    "    #    fully_conn_layer = tf.nn.dropout(fully_conn_layer, 0.75*keep_prob)\n",
    "    \n",
    "    fully_conn_layer1 = fully_conn(fully_conn_layer, 512)\n",
    "    fully_conn_layer2 = tf.nn.dropout(fully_conn_layer1, 0.75*keep_prob)\n",
    "    fully_conn_layer3 = fully_conn(fully_conn_layer2, 256)\n",
    "    fully_conn_layer4 = tf.nn.dropout(fully_conn_layer3, 0.75*keep_prob)\n",
    "    \n",
    "    #full_layer1 = fully_conn(layer4, 10)\n",
    "    #full_layer1 = tf.nn.dropout(full_layer1, keep_prob)\n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    result = output(fully_conn_layer4, 10)\n",
    "    #result = output(full_layer2, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={keep_prob: keep_probability, x: feature_batch, y: label_batch})\n",
    "        \n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    \n",
    "    print('loss: {}, validation_accuracy: {}'.format(loss, validation_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 110\n",
    "batch_size = 256\n",
    "keep_probability = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.1590569019317627, validation_accuracy: 0.20440000295639038\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.1113107204437256, validation_accuracy: 0.24060000479221344\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 1.9567276239395142, validation_accuracy: 0.3075999915599823\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 1.7911357879638672, validation_accuracy: 0.33820000290870667\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 1.79740309715271, validation_accuracy: 0.3425999879837036\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 1.6262671947479248, validation_accuracy: 0.3653999865055084\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.6829874515533447, validation_accuracy: 0.37860000133514404\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.668756127357483, validation_accuracy: 0.3813999891281128\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.4611843824386597, validation_accuracy: 0.39959999918937683\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.380812406539917, validation_accuracy: 0.40220001339912415\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.509030818939209, validation_accuracy: 0.41519999504089355\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.3097738027572632, validation_accuracy: 0.4142000079154968\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.202386736869812, validation_accuracy: 0.42559999227523804\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.2455875873565674, validation_accuracy: 0.42899999022483826\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.2361245155334473, validation_accuracy: 0.45500001311302185\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.1002930402755737, validation_accuracy: 0.4575999975204468\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.0971105098724365, validation_accuracy: 0.44859999418258667\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.2619645595550537, validation_accuracy: 0.4474000036716461\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 0.9331792593002319, validation_accuracy: 0.46880000829696655\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.1277179718017578, validation_accuracy: 0.4578000009059906\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.003189206123352, validation_accuracy: 0.46380001306533813\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 0.9368749856948853, validation_accuracy: 0.4675999879837036\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.005284070968628, validation_accuracy: 0.46799999475479126\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.9119566082954407, validation_accuracy: 0.4779999852180481\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.9173544049263, validation_accuracy: 0.48240000009536743\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.8127667307853699, validation_accuracy: 0.4846000075340271\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.9216896295547485, validation_accuracy: 0.48919999599456787\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.9125720858573914, validation_accuracy: 0.48159998655319214\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.8098607063293457, validation_accuracy: 0.48019999265670776\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.8832806348800659, validation_accuracy: 0.48840001225471497\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.7764095067977905, validation_accuracy: 0.48260000348091125\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.76751309633255, validation_accuracy: 0.48260000348091125\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.6882251501083374, validation_accuracy: 0.5062000155448914\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.7101631760597229, validation_accuracy: 0.49320000410079956\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.7330647110939026, validation_accuracy: 0.4968000054359436\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.7716386914253235, validation_accuracy: 0.4957999885082245\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.7104402184486389, validation_accuracy: 0.49000000953674316\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.6883624792098999, validation_accuracy: 0.501800000667572\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.5801204442977905, validation_accuracy: 0.5\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.7405421733856201, validation_accuracy: 0.48539999127388\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.6816269159317017, validation_accuracy: 0.4957999885082245\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.587620735168457, validation_accuracy: 0.5034000277519226\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.5304113626480103, validation_accuracy: 0.506600022315979\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.5709697604179382, validation_accuracy: 0.5153999924659729\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.6916033625602722, validation_accuracy: 0.4997999966144562\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.5848822593688965, validation_accuracy: 0.5163999795913696\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.5634344220161438, validation_accuracy: 0.5152000188827515\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.5723292231559753, validation_accuracy: 0.5090000033378601\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.6514991521835327, validation_accuracy: 0.5085999965667725\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.6834336519241333, validation_accuracy: 0.5112000107765198\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.4526267647743225, validation_accuracy: 0.5113999843597412\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.5350748896598816, validation_accuracy: 0.5049999952316284\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.47392410039901733, validation_accuracy: 0.5080000162124634\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.5469642877578735, validation_accuracy: 0.5113999843597412\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.5850172638893127, validation_accuracy: 0.5221999883651733\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.45184582471847534, validation_accuracy: 0.506600022315979\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.4949302673339844, validation_accuracy: 0.5180000066757202\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.5262914896011353, validation_accuracy: 0.5252000093460083\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.6021960377693176, validation_accuracy: 0.5156000256538391\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.5439409017562866, validation_accuracy: 0.5026000142097473\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.4420240819454193, validation_accuracy: 0.5049999952316284\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.6378411650657654, validation_accuracy: 0.49480000138282776\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.5215875506401062, validation_accuracy: 0.49959999322891235\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.47424644231796265, validation_accuracy: 0.49619999527931213\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.48569950461387634, validation_accuracy: 0.5105999708175659\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.5625820755958557, validation_accuracy: 0.5144000053405762\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.3730134665966034, validation_accuracy: 0.5230000019073486\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.4394327998161316, validation_accuracy: 0.5221999883651733\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.44123536348342896, validation_accuracy: 0.5257999897003174\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.41089171171188354, validation_accuracy: 0.524399995803833\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.3335355818271637, validation_accuracy: 0.5103999972343445\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.43192920088768005, validation_accuracy: 0.5131999850273132\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.44955024123191833, validation_accuracy: 0.4952000081539154\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.4109016954898834, validation_accuracy: 0.501800000667572\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.4082549214363098, validation_accuracy: 0.48660001158714294\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.4339839816093445, validation_accuracy: 0.504800021648407\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.3883349299430847, validation_accuracy: 0.503000020980835\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.36430805921554565, validation_accuracy: 0.49559998512268066\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.4378553032875061, validation_accuracy: 0.5199999809265137\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.48344120383262634, validation_accuracy: 0.5072000026702881\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.2810215353965759, validation_accuracy: 0.5027999877929688\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.5219225883483887, validation_accuracy: 0.5113999843597412\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.40307945013046265, validation_accuracy: 0.5170000195503235\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.3600999414920807, validation_accuracy: 0.5139999985694885\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.5341053009033203, validation_accuracy: 0.5072000026702881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.5199867486953735, validation_accuracy: 0.5121999979019165\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.3708125948905945, validation_accuracy: 0.5077999830245972\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.39926689863204956, validation_accuracy: 0.5031999945640564\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.3741995394229889, validation_accuracy: 0.5076000094413757\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.38083067536354065, validation_accuracy: 0.5109999775886536\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.3054737448692322, validation_accuracy: 0.5094000101089478\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.21471166610717773, validation_accuracy: 0.5257999897003174\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.38675016164779663, validation_accuracy: 0.527999997138977\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.3120949864387512, validation_accuracy: 0.5288000106811523\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.2778185307979584, validation_accuracy: 0.521399974822998\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.4360816478729248, validation_accuracy: 0.5217999815940857\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.4229782223701477, validation_accuracy: 0.5314000248908997\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.2529422640800476, validation_accuracy: 0.5212000012397766\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.31947264075279236, validation_accuracy: 0.5145999789237976\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.2690652310848236, validation_accuracy: 0.5317999720573425\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss: 0.33239805698394775, validation_accuracy: 0.5325999855995178\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss: 0.2952159643173218, validation_accuracy: 0.5228000283241272\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss: 0.2802359163761139, validation_accuracy: 0.5175999999046326\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss: 0.25447091460227966, validation_accuracy: 0.5228000283241272\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss: 0.3183816075325012, validation_accuracy: 0.5374000072479248\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss: 0.34034615755081177, validation_accuracy: 0.5296000242233276\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss: 0.2959061563014984, validation_accuracy: 0.5239999890327454\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss: 0.2952877879142761, validation_accuracy: 0.5351999998092651\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss: 0.2230272740125656, validation_accuracy: 0.5299999713897705\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss: 0.3094927668571472, validation_accuracy: 0.5270000100135803\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.2679429054260254, validation_accuracy: 0.1111999973654747\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 2.0772390365600586, validation_accuracy: 0.21819999814033508\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 1.9351110458374023, validation_accuracy: 0.23160000145435333\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 1.8254238367080688, validation_accuracy: 0.30140000581741333\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 1.8366997241973877, validation_accuracy: 0.3124000132083893\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 1.8275806903839111, validation_accuracy: 0.3499999940395355\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 1.8688383102416992, validation_accuracy: 0.3529999852180481\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 1.4524723291397095, validation_accuracy: 0.3643999993801117\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 1.6060292720794678, validation_accuracy: 0.3808000087738037\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 1.8559154272079468, validation_accuracy: 0.3815999925136566\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 1.6416828632354736, validation_accuracy: 0.39959999918937683\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 1.6894466876983643, validation_accuracy: 0.41839998960494995\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 1.3820312023162842, validation_accuracy: 0.430400013923645\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 1.5236940383911133, validation_accuracy: 0.4300000071525574\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 1.548789381980896, validation_accuracy: 0.42239999771118164\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 1.644797921180725, validation_accuracy: 0.4456000030040741\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 1.4274126291275024, validation_accuracy: 0.44339999556541443\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 1.235028624534607, validation_accuracy: 0.4505999982357025\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 1.420161485671997, validation_accuracy: 0.44519999623298645\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 1.470733880996704, validation_accuracy: 0.462799996137619\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 1.5722780227661133, validation_accuracy: 0.47200000286102295\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 1.4064676761627197, validation_accuracy: 0.4307999908924103\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 1.2622333765029907, validation_accuracy: 0.4724000096321106\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 1.177818775177002, validation_accuracy: 0.48260000348091125\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 1.3531205654144287, validation_accuracy: 0.462799996137619\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 1.3020187616348267, validation_accuracy: 0.49320000410079956\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 1.3335332870483398, validation_accuracy: 0.4643999934196472\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 1.0753828287124634, validation_accuracy: 0.4733999967575073\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 1.2289594411849976, validation_accuracy: 0.48539999127388\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 1.2513830661773682, validation_accuracy: 0.4765999913215637\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.2828762531280518, validation_accuracy: 0.49399998784065247\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 1.290454626083374, validation_accuracy: 0.4657999873161316\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 1.164638876914978, validation_accuracy: 0.5037999749183655\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 1.032188892364502, validation_accuracy: 0.5040000081062317\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 1.274009346961975, validation_accuracy: 0.4880000054836273\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.2374579906463623, validation_accuracy: 0.4903999865055084\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 1.3281185626983643, validation_accuracy: 0.5063999891281128\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 1.1717793941497803, validation_accuracy: 0.5055999755859375\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 1.144636631011963, validation_accuracy: 0.5138000249862671\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 1.2474628686904907, validation_accuracy: 0.5009999871253967\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.2720367908477783, validation_accuracy: 0.5199999809265137\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 1.1476678848266602, validation_accuracy: 0.5072000026702881\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 1.1876747608184814, validation_accuracy: 0.5228000283241272\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 1.1011228561401367, validation_accuracy: 0.5353999733924866\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 1.0220162868499756, validation_accuracy: 0.519599974155426\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.15744948387146, validation_accuracy: 0.5329999923706055\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 1.1446266174316406, validation_accuracy: 0.5126000046730042\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 1.1123770475387573, validation_accuracy: 0.5320000052452087\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 1.0303068161010742, validation_accuracy: 0.5415999889373779\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 1.0216020345687866, validation_accuracy: 0.5220000147819519\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 0.9359682202339172, validation_accuracy: 0.5342000126838684\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 1.198223352432251, validation_accuracy: 0.5234000086784363\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 0.940933108329773, validation_accuracy: 0.5302000045776367\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 1.058890700340271, validation_accuracy: 0.5437999963760376\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 1.042504072189331, validation_accuracy: 0.531000018119812\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.1882257461547852, validation_accuracy: 0.5406000018119812\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 0.9753732681274414, validation_accuracy: 0.5239999890327454\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 0.8675980567932129, validation_accuracy: 0.5365999937057495\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 1.0271803140640259, validation_accuracy: 0.5432000160217285\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 1.034483790397644, validation_accuracy: 0.5411999821662903\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 0.9486376047134399, validation_accuracy: 0.5424000024795532\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 1.0525875091552734, validation_accuracy: 0.5465999841690063\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 1.0031068325042725, validation_accuracy: 0.5564000010490417\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 0.9052320718765259, validation_accuracy: 0.5568000078201294\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 1.1540203094482422, validation_accuracy: 0.5450000166893005\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 0.9642397165298462, validation_accuracy: 0.5604000091552734\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 1.0663414001464844, validation_accuracy: 0.5523999929428101\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 0.974723219871521, validation_accuracy: 0.5468000173568726\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 0.9224519729614258, validation_accuracy: 0.5626000165939331\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 0.9878925085067749, validation_accuracy: 0.5564000010490417\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.077172875404358, validation_accuracy: 0.5594000220298767\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 0.9563360214233398, validation_accuracy: 0.5419999957084656\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 0.9136694669723511, validation_accuracy: 0.551800012588501\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 0.8563047647476196, validation_accuracy: 0.5691999793052673\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 0.9184747934341431, validation_accuracy: 0.5734000205993652\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 0.9128177762031555, validation_accuracy: 0.5681999921798706\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 0.975182831287384, validation_accuracy: 0.5490000247955322\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 0.8319879770278931, validation_accuracy: 0.5753999948501587\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 0.933100700378418, validation_accuracy: 0.5582000017166138\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 0.9438852071762085, validation_accuracy: 0.5630000233650208\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.0011928081512451, validation_accuracy: 0.5681999921798706\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 1.103132963180542, validation_accuracy: 0.5813999772071838\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 0.8574417233467102, validation_accuracy: 0.578000009059906\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 0.9373947978019714, validation_accuracy: 0.5627999901771545\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 0.9572238922119141, validation_accuracy: 0.5703999996185303\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 0.959675133228302, validation_accuracy: 0.5785999894142151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, CIFAR-10 Batch 2:  loss: 1.0468063354492188, validation_accuracy: 0.5568000078201294\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 0.7626568078994751, validation_accuracy: 0.5627999901771545\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 0.8254993557929993, validation_accuracy: 0.5758000016212463\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 0.8798589706420898, validation_accuracy: 0.5734000205993652\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.0403683185577393, validation_accuracy: 0.5752000212669373\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 1.0987117290496826, validation_accuracy: 0.574999988079071\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 0.756130576133728, validation_accuracy: 0.5781999826431274\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 0.669859766960144, validation_accuracy: 0.579800009727478\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 0.7659553289413452, validation_accuracy: 0.5834000110626221\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 0.9605697393417358, validation_accuracy: 0.5813999772071838\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 0.9474509954452515, validation_accuracy: 0.5774000287055969\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 0.6715731620788574, validation_accuracy: 0.5705999732017517\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 0.7330206036567688, validation_accuracy: 0.5723999738693237\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 0.8018110394477844, validation_accuracy: 0.5806000232696533\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.0265244245529175, validation_accuracy: 0.5824000239372253\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 0.8242559432983398, validation_accuracy: 0.5685999989509583\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 0.7264506816864014, validation_accuracy: 0.5856000185012817\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 0.8379934430122375, validation_accuracy: 0.5863999724388123\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 0.8268902897834778, validation_accuracy: 0.573199987411499\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 0.8914633989334106, validation_accuracy: 0.5917999744415283\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 0.8074970245361328, validation_accuracy: 0.5843999981880188\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 0.9303390383720398, validation_accuracy: 0.5871999859809875\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 0.7882367372512817, validation_accuracy: 0.5884000062942505\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 0.7312801480293274, validation_accuracy: 0.5929999947547913\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 0.851941704750061, validation_accuracy: 0.5881999731063843\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 0.8650544285774231, validation_accuracy: 0.5965999960899353\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 0.756841778755188, validation_accuracy: 0.5770000219345093\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 0.6099483370780945, validation_accuracy: 0.6007999777793884\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 0.6891440749168396, validation_accuracy: 0.5716000199317932\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.7229056358337402, validation_accuracy: 0.5938000082969666\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 0.8997416496276855, validation_accuracy: 0.5893999934196472\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 0.6997534036636353, validation_accuracy: 0.6029999852180481\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 0.751528263092041, validation_accuracy: 0.5914000272750854\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 0.8437213897705078, validation_accuracy: 0.5870000123977661\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.8543723821640015, validation_accuracy: 0.5989999771118164\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 0.7639349102973938, validation_accuracy: 0.6025999784469604\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 0.6735677123069763, validation_accuracy: 0.6015999913215637\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 0.724021315574646, validation_accuracy: 0.5974000096321106\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 0.74326092004776, validation_accuracy: 0.6010000109672546\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.8592969179153442, validation_accuracy: 0.6025999784469604\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 0.962884783744812, validation_accuracy: 0.6075999736785889\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 0.7885205149650574, validation_accuracy: 0.6033999919891357\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 0.6174467206001282, validation_accuracy: 0.5839999914169312\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 0.7852984666824341, validation_accuracy: 0.5871999859809875\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.6787399053573608, validation_accuracy: 0.5996000170707703\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 0.7767602801322937, validation_accuracy: 0.5968000292778015\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 0.6289933919906616, validation_accuracy: 0.5953999757766724\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 0.5634886622428894, validation_accuracy: 0.6000000238418579\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 0.8628376126289368, validation_accuracy: 0.5914000272750854\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.7798441648483276, validation_accuracy: 0.6029999852180481\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 0.7668722867965698, validation_accuracy: 0.6039999723434448\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 0.6312768459320068, validation_accuracy: 0.6150000095367432\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 0.6227941513061523, validation_accuracy: 0.5867999792098999\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 0.8125486373901367, validation_accuracy: 0.6111999750137329\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.7511321306228638, validation_accuracy: 0.6172000169754028\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 0.7777148485183716, validation_accuracy: 0.6093999743461609\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 0.6007042527198792, validation_accuracy: 0.6161999702453613\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 0.5985336899757385, validation_accuracy: 0.604200005531311\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 0.7414453029632568, validation_accuracy: 0.6079999804496765\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.7872955203056335, validation_accuracy: 0.6100000143051147\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 0.7255278825759888, validation_accuracy: 0.6037999987602234\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 0.6640115976333618, validation_accuracy: 0.5956000089645386\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 0.70384281873703, validation_accuracy: 0.6092000007629395\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 0.6296533346176147, validation_accuracy: 0.6140000224113464\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.7860574126243591, validation_accuracy: 0.6057999730110168\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 0.6686615347862244, validation_accuracy: 0.61080002784729\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 0.6513816714286804, validation_accuracy: 0.6150000095367432\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 0.5769049525260925, validation_accuracy: 0.6123999953269958\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 0.7206150889396667, validation_accuracy: 0.6195999979972839\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.7230150103569031, validation_accuracy: 0.6123999953269958\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 0.7731420397758484, validation_accuracy: 0.61080002784729\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 0.5780020952224731, validation_accuracy: 0.6133999824523926\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 0.6145356297492981, validation_accuracy: 0.6227999925613403\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 0.7152965664863586, validation_accuracy: 0.5953999757766724\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.7628270387649536, validation_accuracy: 0.6050000190734863\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 0.7118436694145203, validation_accuracy: 0.6150000095367432\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 0.505994975566864, validation_accuracy: 0.6101999878883362\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 0.5134453773498535, validation_accuracy: 0.616599977016449\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 0.6494747996330261, validation_accuracy: 0.6191999912261963\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.627627968788147, validation_accuracy: 0.6018000245094299\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 0.7896229028701782, validation_accuracy: 0.6050000190734863\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 0.6028062701225281, validation_accuracy: 0.61080002784729\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 0.6581862568855286, validation_accuracy: 0.6240000128746033\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 0.7349125146865845, validation_accuracy: 0.6136000156402588\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.7012225389480591, validation_accuracy: 0.6291999816894531\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 0.6955205798149109, validation_accuracy: 0.6255999803543091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, CIFAR-10 Batch 3:  loss: 0.514215350151062, validation_accuracy: 0.6032000184059143\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 0.6387948989868164, validation_accuracy: 0.5979999899864197\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 0.6002745032310486, validation_accuracy: 0.6128000020980835\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.7440235614776611, validation_accuracy: 0.6204000115394592\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 0.7221468091011047, validation_accuracy: 0.6212000250816345\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 0.6432034373283386, validation_accuracy: 0.6126000285148621\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 0.5874031782150269, validation_accuracy: 0.6308000087738037\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 0.6655756235122681, validation_accuracy: 0.6144000291824341\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.6805587410926819, validation_accuracy: 0.6190000176429749\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 0.8105963468551636, validation_accuracy: 0.6126000285148621\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 0.6874986886978149, validation_accuracy: 0.621399998664856\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 0.6047569513320923, validation_accuracy: 0.6032000184059143\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 0.590722918510437, validation_accuracy: 0.6200000047683716\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.8186008334159851, validation_accuracy: 0.6230000257492065\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 0.6645715832710266, validation_accuracy: 0.6161999702453613\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 0.5535827279090881, validation_accuracy: 0.6126000285148621\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 0.6485568881034851, validation_accuracy: 0.6215999722480774\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 0.5761263966560364, validation_accuracy: 0.6128000020980835\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.7348672151565552, validation_accuracy: 0.6266000270843506\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 0.7654114365577698, validation_accuracy: 0.6259999871253967\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 0.4707546830177307, validation_accuracy: 0.6150000095367432\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 0.42831605672836304, validation_accuracy: 0.6255999803543091\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 0.513160228729248, validation_accuracy: 0.6291999816894531\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.8152999877929688, validation_accuracy: 0.6367999911308289\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 0.7149736285209656, validation_accuracy: 0.6176000237464905\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 0.5988956689834595, validation_accuracy: 0.6259999871253967\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 0.5252465009689331, validation_accuracy: 0.6233999729156494\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 0.6549439430236816, validation_accuracy: 0.6240000128746033\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.58916175365448, validation_accuracy: 0.6326000094413757\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 0.7253867387771606, validation_accuracy: 0.6155999898910522\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 0.5183297395706177, validation_accuracy: 0.6317999958992004\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 0.5350459218025208, validation_accuracy: 0.6389999985694885\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 0.4859910011291504, validation_accuracy: 0.6223999857902527\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.6250964403152466, validation_accuracy: 0.6218000054359436\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 0.7380256652832031, validation_accuracy: 0.6191999912261963\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 0.5866224765777588, validation_accuracy: 0.6340000033378601\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 0.5279859304428101, validation_accuracy: 0.6164000034332275\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 0.6098640561103821, validation_accuracy: 0.6215999722480774\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.6090049147605896, validation_accuracy: 0.6326000094413757\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 0.745698094367981, validation_accuracy: 0.6254000067710876\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 0.5426085591316223, validation_accuracy: 0.623199999332428\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 0.5511388182640076, validation_accuracy: 0.6308000087738037\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 0.5014619827270508, validation_accuracy: 0.6313999891281128\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.6128348112106323, validation_accuracy: 0.6248000264167786\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 0.6274164915084839, validation_accuracy: 0.6251999735832214\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 0.4130023419857025, validation_accuracy: 0.6262000203132629\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 0.5460968017578125, validation_accuracy: 0.6276000142097473\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 0.6236015558242798, validation_accuracy: 0.6204000115394592\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.7202529907226562, validation_accuracy: 0.6287999749183655\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 0.5914794206619263, validation_accuracy: 0.6370000243186951\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 0.473006010055542, validation_accuracy: 0.633400022983551\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 0.3956288695335388, validation_accuracy: 0.6362000107765198\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 0.6555154919624329, validation_accuracy: 0.6168000102043152\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.48988422751426697, validation_accuracy: 0.6291999816894531\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 0.8135139346122742, validation_accuracy: 0.626800000667572\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 0.6645213961601257, validation_accuracy: 0.6237999796867371\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 0.5228375196456909, validation_accuracy: 0.6335999965667725\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 0.44933444261550903, validation_accuracy: 0.6388000249862671\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.6683815121650696, validation_accuracy: 0.6323999762535095\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 0.6630443334579468, validation_accuracy: 0.6259999871253967\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 0.5222116112709045, validation_accuracy: 0.6215999722480774\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 0.46150508522987366, validation_accuracy: 0.6373999714851379\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 0.5205444693565369, validation_accuracy: 0.6302000284194946\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.6380847096443176, validation_accuracy: 0.629800021648407\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 0.7289641499519348, validation_accuracy: 0.6366000175476074\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 0.5122981071472168, validation_accuracy: 0.6255999803543091\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 0.40148019790649414, validation_accuracy: 0.6226000189781189\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 0.6673912405967712, validation_accuracy: 0.6227999925613403\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.6170822381973267, validation_accuracy: 0.633400022983551\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 0.5985037088394165, validation_accuracy: 0.6290000081062317\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 0.57606041431427, validation_accuracy: 0.6241999864578247\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 0.44442206621170044, validation_accuracy: 0.6484000086784363\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 0.5047005414962769, validation_accuracy: 0.6262000203132629\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.7561860680580139, validation_accuracy: 0.6191999912261963\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 0.7226734161376953, validation_accuracy: 0.6259999871253967\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 0.4127681851387024, validation_accuracy: 0.6353999972343445\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 0.4554023742675781, validation_accuracy: 0.6327999830245972\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 0.5210393667221069, validation_accuracy: 0.6340000033378601\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.5641108751296997, validation_accuracy: 0.6236000061035156\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 0.6303591132164001, validation_accuracy: 0.6255999803543091\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 0.5829100608825684, validation_accuracy: 0.6335999965667725\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 0.5200501680374146, validation_accuracy: 0.6466000080108643\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 0.43526023626327515, validation_accuracy: 0.640999972820282\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.6295450925827026, validation_accuracy: 0.6323999762535095\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 0.5805414915084839, validation_accuracy: 0.6305999755859375\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 0.448164701461792, validation_accuracy: 0.6384000182151794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, CIFAR-10 Batch 4:  loss: 0.47579890489578247, validation_accuracy: 0.621999979019165\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 0.6141635179519653, validation_accuracy: 0.6394000053405762\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.537386417388916, validation_accuracy: 0.6233999729156494\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 0.47009649872779846, validation_accuracy: 0.6467999815940857\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 0.44790077209472656, validation_accuracy: 0.626800000667572\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 0.4439653754234314, validation_accuracy: 0.6406000256538391\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 0.4352814257144928, validation_accuracy: 0.6424000263214111\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.5776236653327942, validation_accuracy: 0.6331999897956848\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 0.572192370891571, validation_accuracy: 0.6323999762535095\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 0.48198533058166504, validation_accuracy: 0.6362000107765198\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 0.5871360898017883, validation_accuracy: 0.6448000073432922\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 0.5867844820022583, validation_accuracy: 0.6380000114440918\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.5650848150253296, validation_accuracy: 0.631600022315979\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 0.5868175625801086, validation_accuracy: 0.6421999931335449\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 0.38428014516830444, validation_accuracy: 0.6295999884605408\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 0.5774043202400208, validation_accuracy: 0.6363999843597412\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 0.5549817085266113, validation_accuracy: 0.6444000005722046\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.6132439374923706, validation_accuracy: 0.6309999823570251\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 0.503825306892395, validation_accuracy: 0.6395999789237976\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 0.40312203764915466, validation_accuracy: 0.6373999714851379\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 0.44163060188293457, validation_accuracy: 0.644599974155426\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 0.5347132682800293, validation_accuracy: 0.6394000053405762\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.5842572450637817, validation_accuracy: 0.628000020980835\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 0.729570209980011, validation_accuracy: 0.6367999911308289\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 0.4462551176548004, validation_accuracy: 0.6284000277519226\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 0.5351252555847168, validation_accuracy: 0.6434000134468079\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 0.4543423652648926, validation_accuracy: 0.6417999863624573\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.6588751077651978, validation_accuracy: 0.6344000101089478\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 0.6282666921615601, validation_accuracy: 0.6398000121116638\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 0.4665297567844391, validation_accuracy: 0.6359999775886536\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 0.4397900700569153, validation_accuracy: 0.6377999782562256\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 0.48855456709861755, validation_accuracy: 0.6388000249862671\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.6089652180671692, validation_accuracy: 0.635200023651123\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 0.49521273374557495, validation_accuracy: 0.6466000080108643\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 0.4794372022151947, validation_accuracy: 0.6388000249862671\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 0.46507272124290466, validation_accuracy: 0.6385999917984009\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 0.4577564299106598, validation_accuracy: 0.6478000283241272\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.6312128901481628, validation_accuracy: 0.6123999953269958\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 0.5821226239204407, validation_accuracy: 0.6417999863624573\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 0.36505594849586487, validation_accuracy: 0.6425999999046326\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 0.4548454284667969, validation_accuracy: 0.6484000086784363\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 0.5892488956451416, validation_accuracy: 0.6507999897003174\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.5637784004211426, validation_accuracy: 0.6323999762535095\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 0.3953200578689575, validation_accuracy: 0.6388000249862671\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 0.508123517036438, validation_accuracy: 0.6407999992370605\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 0.6146584749221802, validation_accuracy: 0.6435999870300293\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 0.4775920808315277, validation_accuracy: 0.6370000243186951\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.5376321077346802, validation_accuracy: 0.6258000135421753\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 0.47897544503211975, validation_accuracy: 0.647599995136261\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 0.4355430006980896, validation_accuracy: 0.659600019454956\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 0.3800823986530304, validation_accuracy: 0.6467999815940857\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 0.4822555482387543, validation_accuracy: 0.6385999917984009\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.6507505178451538, validation_accuracy: 0.6402000188827515\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 0.49012240767478943, validation_accuracy: 0.647599995136261\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 0.6115512251853943, validation_accuracy: 0.6466000080108643\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 0.4785180985927582, validation_accuracy: 0.6503999829292297\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 0.49093085527420044, validation_accuracy: 0.6480000019073486\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.5398645997047424, validation_accuracy: 0.635200023651123\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 0.42246705293655396, validation_accuracy: 0.6439999938011169\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 0.45395708084106445, validation_accuracy: 0.6435999870300293\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 0.4258348345756531, validation_accuracy: 0.6517999768257141\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.44157320261001587, validation_accuracy: 0.656000018119812\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.5411661863327026, validation_accuracy: 0.6366000175476074\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 0.4919944703578949, validation_accuracy: 0.6438000202178955\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 0.3705040216445923, validation_accuracy: 0.644599974155426\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 0.37019675970077515, validation_accuracy: 0.6388000249862671\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 0.4181477427482605, validation_accuracy: 0.646399974822998\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.6009408235549927, validation_accuracy: 0.6312000155448914\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 0.4736996293067932, validation_accuracy: 0.6492000222206116\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 0.45019498467445374, validation_accuracy: 0.6412000060081482\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 0.4521949291229248, validation_accuracy: 0.6517999768257141\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.4355182647705078, validation_accuracy: 0.6520000100135803\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.5046054720878601, validation_accuracy: 0.652999997138977\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.5376198291778564, validation_accuracy: 0.6394000053405762\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 0.3910443186759949, validation_accuracy: 0.6471999883651733\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 0.4669763147830963, validation_accuracy: 0.6514000296592712\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.4534595012664795, validation_accuracy: 0.6402000188827515\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.6307234764099121, validation_accuracy: 0.6456000208854675\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 0.6887985467910767, validation_accuracy: 0.6456000208854675\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 0.474921315908432, validation_accuracy: 0.6601999998092651\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 0.4389837682247162, validation_accuracy: 0.6547999978065491\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.32427430152893066, validation_accuracy: 0.6582000255584717\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.5621668696403503, validation_accuracy: 0.631600022315979\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.3841586709022522, validation_accuracy: 0.6416000127792358\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 0.4496075510978699, validation_accuracy: 0.6269999742507935\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 0.40991997718811035, validation_accuracy: 0.6412000060081482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.39360302686691284, validation_accuracy: 0.6395999789237976\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.49828213453292847, validation_accuracy: 0.6417999863624573\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 0.4069437086582184, validation_accuracy: 0.6510000228881836\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 0.6039050221443176, validation_accuracy: 0.656000018119812\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 0.5396321415901184, validation_accuracy: 0.6585999727249146\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.44271379709243774, validation_accuracy: 0.650600016117096\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.569436252117157, validation_accuracy: 0.6431999802589417\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 0.4220140874385834, validation_accuracy: 0.6514000296592712\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.43696650862693787, validation_accuracy: 0.6585999727249146\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 0.3575232923030853, validation_accuracy: 0.6585999727249146\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.46972355246543884, validation_accuracy: 0.652400016784668\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.5196620225906372, validation_accuracy: 0.6424000263214111\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 0.45121684670448303, validation_accuracy: 0.6624000072479248\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.5652638673782349, validation_accuracy: 0.6534000039100647\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 0.45080310106277466, validation_accuracy: 0.6478000283241272\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.4796485900878906, validation_accuracy: 0.6514000296592712\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.48810014128685, validation_accuracy: 0.6539999842643738\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.4372350573539734, validation_accuracy: 0.6498000025749207\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 0.36795860528945923, validation_accuracy: 0.6448000073432922\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 0.3211044371128082, validation_accuracy: 0.6462000012397766\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.5109232664108276, validation_accuracy: 0.652999997138977\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.618694007396698, validation_accuracy: 0.6435999870300293\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.4925941526889801, validation_accuracy: 0.6353999972343445\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.40125709772109985, validation_accuracy: 0.6389999985694885\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 0.4077877104282379, validation_accuracy: 0.6606000065803528\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.43948835134506226, validation_accuracy: 0.6416000127792358\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.5266324877738953, validation_accuracy: 0.6413999795913696\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.3972592353820801, validation_accuracy: 0.6507999897003174\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.2925194203853607, validation_accuracy: 0.6448000073432922\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 0.48064202070236206, validation_accuracy: 0.6565999984741211\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.365507036447525, validation_accuracy: 0.644599974155426\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.5107653737068176, validation_accuracy: 0.6302000284194946\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.5736142992973328, validation_accuracy: 0.6412000060081482\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.4723171293735504, validation_accuracy: 0.644599974155426\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 0.33680376410484314, validation_accuracy: 0.6438000202178955\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.4016622006893158, validation_accuracy: 0.6489999890327454\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.5166230201721191, validation_accuracy: 0.6353999972343445\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.504200279712677, validation_accuracy: 0.6420000195503235\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.3409549295902252, validation_accuracy: 0.6485999822616577\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 0.43588560819625854, validation_accuracy: 0.6593999862670898\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.5055506229400635, validation_accuracy: 0.6291999816894531\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.6225188374519348, validation_accuracy: 0.65420001745224\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.483222097158432, validation_accuracy: 0.6399999856948853\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.48851102590560913, validation_accuracy: 0.6416000127792358\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 0.4134196639060974, validation_accuracy: 0.6650000214576721\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.4949158728122711, validation_accuracy: 0.6449999809265137\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.6275529265403748, validation_accuracy: 0.6651999950408936\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.4840005040168762, validation_accuracy: 0.6549999713897705\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.31851285696029663, validation_accuracy: 0.6535999774932861\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 0.39750543236732483, validation_accuracy: 0.6592000126838684\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.39945167303085327, validation_accuracy: 0.6521999835968018\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.524756133556366, validation_accuracy: 0.6567999720573425\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.40145978331565857, validation_accuracy: 0.6413999795913696\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.3961733877658844, validation_accuracy: 0.6534000039100647\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 0.4210919439792633, validation_accuracy: 0.6517999768257141\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.39971309900283813, validation_accuracy: 0.6496000289916992\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.5149869322776794, validation_accuracy: 0.6575999855995178\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 0.5020259022712708, validation_accuracy: 0.6380000114440918\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 0.3735690116882324, validation_accuracy: 0.6467999815940857\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 0.4571727216243744, validation_accuracy: 0.6656000018119812\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 0.39529240131378174, validation_accuracy: 0.6467999815940857\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.49845752120018005, validation_accuracy: 0.6430000066757202\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 0.5792580246925354, validation_accuracy: 0.6498000025749207\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 0.3465753495693207, validation_accuracy: 0.6553999781608582\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 0.497140109539032, validation_accuracy: 0.6561999917030334\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 0.5736976861953735, validation_accuracy: 0.6561999917030334\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.4777553081512451, validation_accuracy: 0.6546000242233276\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 0.5179112553596497, validation_accuracy: 0.6373999714851379\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 0.3962974548339844, validation_accuracy: 0.65420001745224\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 0.4514434337615967, validation_accuracy: 0.6420000195503235\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 0.3886811137199402, validation_accuracy: 0.6557999849319458\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.5588365793228149, validation_accuracy: 0.6547999978065491\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 0.5174268484115601, validation_accuracy: 0.6366000175476074\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 0.4638919234275818, validation_accuracy: 0.6481999754905701\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 0.3621289134025574, validation_accuracy: 0.6546000242233276\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 0.5006787180900574, validation_accuracy: 0.6531999707221985\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.429564893245697, validation_accuracy: 0.647599995136261\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 0.5801359415054321, validation_accuracy: 0.6615999937057495\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 0.3361395299434662, validation_accuracy: 0.650600016117096\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 0.3500339090824127, validation_accuracy: 0.6553999781608582\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 0.596711277961731, validation_accuracy: 0.6380000114440918\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.5142427682876587, validation_accuracy: 0.65420001745224\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 0.39139804244041443, validation_accuracy: 0.6583999991416931\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 0.4974840581417084, validation_accuracy: 0.640999972820282\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 0.48543182015419006, validation_accuracy: 0.6611999869346619\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss: 0.34150657057762146, validation_accuracy: 0.6462000012397766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.5821623206138611, validation_accuracy: 0.6668000221252441\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 0.4339459538459778, validation_accuracy: 0.6488000154495239\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 0.3433445990085602, validation_accuracy: 0.6556000113487244\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 0.3083289563655853, validation_accuracy: 0.6610000133514404\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 0.3850560784339905, validation_accuracy: 0.652999997138977\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.4997130334377289, validation_accuracy: 0.6628000140190125\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 0.5625472068786621, validation_accuracy: 0.6579999923706055\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 0.50774085521698, validation_accuracy: 0.645799994468689\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 0.46898335218429565, validation_accuracy: 0.6579999923706055\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 0.4604128897190094, validation_accuracy: 0.6460000276565552\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.6036173105239868, validation_accuracy: 0.6499999761581421\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 0.4856886863708496, validation_accuracy: 0.6489999890327454\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 0.48715949058532715, validation_accuracy: 0.6629999876022339\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 0.45497655868530273, validation_accuracy: 0.6588000059127808\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 0.3162975311279297, validation_accuracy: 0.6534000039100647\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.4893408715724945, validation_accuracy: 0.6610000133514404\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 0.3894059658050537, validation_accuracy: 0.6485999822616577\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 0.3562649190425873, validation_accuracy: 0.6521999835968018\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 0.3809683918952942, validation_accuracy: 0.6633999943733215\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 0.44493311643600464, validation_accuracy: 0.646399974822998\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.6024211645126343, validation_accuracy: 0.6478000283241272\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 0.49554720520973206, validation_accuracy: 0.6460000276565552\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 0.3838964104652405, validation_accuracy: 0.6467999815940857\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 0.3073366582393646, validation_accuracy: 0.6629999876022339\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 0.3835502564907074, validation_accuracy: 0.6452000141143799\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.638840913772583, validation_accuracy: 0.6592000126838684\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 0.5196658372879028, validation_accuracy: 0.6514000296592712\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 0.4351338744163513, validation_accuracy: 0.6510000228881836\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 0.43597421050071716, validation_accuracy: 0.6647999882698059\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 0.5088468790054321, validation_accuracy: 0.6489999890327454\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.42828574776649475, validation_accuracy: 0.6489999890327454\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 0.40793246030807495, validation_accuracy: 0.6565999984741211\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 0.3312187194824219, validation_accuracy: 0.6657999753952026\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 0.3594897985458374, validation_accuracy: 0.6643999814987183\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 0.3746728301048279, validation_accuracy: 0.6547999978065491\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.4854680895805359, validation_accuracy: 0.6521999835968018\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 0.3829457461833954, validation_accuracy: 0.6448000073432922\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 0.2685154974460602, validation_accuracy: 0.6556000113487244\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 0.3896245062351227, validation_accuracy: 0.6723999977111816\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 0.48487478494644165, validation_accuracy: 0.6593999862670898\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.5003392696380615, validation_accuracy: 0.6531999707221985\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 0.3918895125389099, validation_accuracy: 0.6549999713897705\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 0.38616034388542175, validation_accuracy: 0.657800018787384\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 0.4089820981025696, validation_accuracy: 0.6639999747276306\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 0.3734204173088074, validation_accuracy: 0.6657999753952026\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.49911561608314514, validation_accuracy: 0.6431999802589417\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 0.47972744703292847, validation_accuracy: 0.646399974822998\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 0.36943021416664124, validation_accuracy: 0.6633999943733215\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 0.34253591299057007, validation_accuracy: 0.6729999780654907\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 0.43949899077415466, validation_accuracy: 0.6553999781608582\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.4621369242668152, validation_accuracy: 0.6582000255584717\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 0.47650495171546936, validation_accuracy: 0.6610000133514404\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 0.3352941870689392, validation_accuracy: 0.6489999890327454\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 0.3374493718147278, validation_accuracy: 0.6718000173568726\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 0.37059158086776733, validation_accuracy: 0.6565999984741211\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.5900385975837708, validation_accuracy: 0.6510000228881836\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 0.42472848296165466, validation_accuracy: 0.646399974822998\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 0.36019372940063477, validation_accuracy: 0.6376000046730042\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 0.3726497292518616, validation_accuracy: 0.6535999774932861\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 0.4761929512023926, validation_accuracy: 0.6597999930381775\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.49047422409057617, validation_accuracy: 0.6431999802589417\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 0.46154946088790894, validation_accuracy: 0.6444000005722046\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 0.383661687374115, validation_accuracy: 0.6552000045776367\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 0.26765528321266174, validation_accuracy: 0.6585999727249146\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 0.40038877725601196, validation_accuracy: 0.6492000222206116\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.49716177582740784, validation_accuracy: 0.6434000134468079\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 0.4144652783870697, validation_accuracy: 0.652999997138977\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 0.3127431273460388, validation_accuracy: 0.6588000059127808\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 0.41942667961120605, validation_accuracy: 0.6687999963760376\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 0.2526960074901581, validation_accuracy: 0.6600000262260437\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss: 0.4688591957092285, validation_accuracy: 0.6725999712944031\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss: 0.5455323457717896, validation_accuracy: 0.6546000242233276\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss: 0.365999311208725, validation_accuracy: 0.646399974822998\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss: 0.5334733128547668, validation_accuracy: 0.6650000214576721\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss: 0.3199746012687683, validation_accuracy: 0.6651999950408936\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss: 0.5942843556404114, validation_accuracy: 0.6607999801635742\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss: 0.44390591979026794, validation_accuracy: 0.6664000153541565\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss: 0.381334513425827, validation_accuracy: 0.651199996471405\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss: 0.36078158020973206, validation_accuracy: 0.6705999970436096\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss: 0.42766112089157104, validation_accuracy: 0.6520000100135803\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss: 0.4177854657173157, validation_accuracy: 0.670199990272522\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss: 0.42093682289123535, validation_accuracy: 0.6665999889373779\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss: 0.2805113196372986, validation_accuracy: 0.6561999917030334\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss: 0.39792191982269287, validation_accuracy: 0.6729999780654907\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss: 0.42083925008773804, validation_accuracy: 0.6692000031471252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104, CIFAR-10 Batch 1:  loss: 0.48639097809791565, validation_accuracy: 0.6718000173568726\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss: 0.39062103629112244, validation_accuracy: 0.6610000133514404\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss: 0.345883846282959, validation_accuracy: 0.6435999870300293\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss: 0.31744250655174255, validation_accuracy: 0.6636000275611877\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss: 0.296773761510849, validation_accuracy: 0.659600019454956\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss: 0.40232715010643005, validation_accuracy: 0.6582000255584717\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss: 0.5123485326766968, validation_accuracy: 0.6556000113487244\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss: 0.2720915675163269, validation_accuracy: 0.6498000025749207\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss: 0.42728251218795776, validation_accuracy: 0.675000011920929\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss: 0.4263669550418854, validation_accuracy: 0.6687999963760376\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss: 0.4099002778530121, validation_accuracy: 0.675000011920929\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss: 0.44686374068260193, validation_accuracy: 0.6661999821662903\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss: 0.2739359438419342, validation_accuracy: 0.6597999930381775\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss: 0.5119507908821106, validation_accuracy: 0.6574000120162964\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss: 0.3489765524864197, validation_accuracy: 0.6674000024795532\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss: 0.4859544634819031, validation_accuracy: 0.6654000282287598\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss: 0.39142411947250366, validation_accuracy: 0.6535999774932861\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss: 0.3807935118675232, validation_accuracy: 0.6601999998092651\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss: 0.40362221002578735, validation_accuracy: 0.675599992275238\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss: 0.335130512714386, validation_accuracy: 0.671999990940094\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss: 0.4248684048652649, validation_accuracy: 0.6565999984741211\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss: 0.4866926670074463, validation_accuracy: 0.6539999842643738\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss: 0.37953922152519226, validation_accuracy: 0.670199990272522\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss: 0.31639400124549866, validation_accuracy: 0.6700000166893005\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss: 0.2903197109699249, validation_accuracy: 0.6696000099182129\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss: 0.4017588496208191, validation_accuracy: 0.6618000268936157\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss: 0.40628498792648315, validation_accuracy: 0.6606000065803528\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss: 0.3617170453071594, validation_accuracy: 0.6751999855041504\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss: 0.3298255503177643, validation_accuracy: 0.6759999990463257\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss: 0.376132607460022, validation_accuracy: 0.6592000126838684\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss: 0.3930356502532959, validation_accuracy: 0.6583999991416931\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss: 0.3788811266422272, validation_accuracy: 0.6642000079154968\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss: 0.33279234170913696, validation_accuracy: 0.6647999882698059\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss: 0.43947601318359375, validation_accuracy: 0.6615999937057495\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss: 0.5082389116287231, validation_accuracy: 0.6728000044822693\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6611328125\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5f4cec550>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
